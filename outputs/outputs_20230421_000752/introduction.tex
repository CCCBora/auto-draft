\section{introduction}
Reinforcement Learning (RL) has emerged as a significant research area in the field of artificial intelligence, with a wide range of applications in robotics, finance, healthcare, and gaming \cite{2108.11510}. The primary goal of RL is to develop algorithms that allow agents to learn optimal policies through interaction with their environment, maximizing the cumulative reward over time \cite{1708.05866}. Despite the considerable progress made in recent years, RL still faces several challenges, such as the trade-off between exploration and exploitation, the curse of dimensionality, and the need for efficient algorithms that can handle large-scale and complex problems \cite{1906.10025}.

One of the major breakthroughs in RL has been the development of Q-learning algorithms, which have been proven to converge to the optimal solution \cite{2303.08631}. However, Q-learning is known to suffer from overestimation bias, leading to suboptimal performance and slow convergence in some cases \cite{2106.14642}. To address this issue, researchers have proposed various modifications and extensions to Q-learning, such as Double Q-learning \cite{1511.02377} and Self-correcting Q-learning \cite{2012.01100}, which aim to mitigate the overestimation bias while maintaining convergence guarantees.

Another essential aspect of RL research is the incorporation of deep learning techniques, giving rise to the field of Deep Reinforcement Learning (DRL) \cite{1709.05067}. DRL has demonstrated remarkable success in various domains, such as playing video games directly from pixels and learning control policies for robots \cite{1708.05866}. However, DRL algorithms often require a large amount of data and computational resources, which limits their applicability in real-world scenarios \cite{1906.10025}. To overcome these limitations, researchers have proposed various approaches, including distributed DRL \cite{2212.00253} and expert-guided DRL \cite{2106.14642}, which aim to improve the sample efficiency and scalability of DRL algorithms.

Related work in the field of RL has also focused on the development of policy gradient methods, which optimize the policy directly by following the gradient of the expected return \cite{1811.09013}. These methods have been particularly successful in continuous action settings and have led to the development of algorithms such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) \cite{2209.01820}. However, policy gradient methods often require on-policy data, which can be inefficient in terms of sample complexity \cite{1911.04817}.

In summary, this survey aims to provide a comprehensive overview of the current state of Reinforcement Learning, focusing on the challenges and recent advances in Q-learning, Deep Reinforcement Learning, and policy gradient methods. By examining the key algorithms, techniques, and applications in these areas, we hope to shed light on the current limitations and future research directions in the field of RL.