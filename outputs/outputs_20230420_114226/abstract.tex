\begin{abstract}In this paper, we propose a novel approach to training adversarial generative neural networks using an adaptive dropout rate, which aims to address the overfitting issue and improve the performance of deep neural networks (DNNs) in various applications. Our method extends traditional dropout methods by incorporating an adaptive dropout rate that is sensitive to the input data, enabling the resulting network to tolerate a higher degree of sparsity without losing its expressive power. We demonstrate the effectiveness of our approach on a variety of applications, including image generation, text classification, and regression, showing that our method outperforms existing dropout techniques in terms of accuracy and robustness. Our research contributes to the ongoing efforts to improve the performance and robustness of deep learning models, particularly adversarial generative neural networks, and offers a promising solution for training more robust and accurate deep learning models in various applications.\end{abstract}